{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a5a6eb-c323-4cac-9e12-310a41f0bac3",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42da1f69-fc0a-4caa-86e5-ecf656813efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0d5d40a6a37e:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1636940821654)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(ItemID,IntegerType,false), StructField(Sentiment,IntegerType,false), StructField(SentimentText,StringType,false))\n",
       "data: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = StructType(\"ItemID Sentiment SentimentText\"\n",
    "                        .split(\" \")\n",
    "                        .map(fieldName => {\n",
    "                            if (fieldName == \"ItemID\" || fieldName == \"Sentiment\")\n",
    "                                StructField(fieldName, IntegerType, nullable = false)\n",
    "                            else\n",
    "                                StructField(fieldName, StringType, nullable = false)\n",
    "                        }))\n",
    "\n",
    "val data = spark.read.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"twitter_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172e0bf6-0926-44d6-81e1-f74b2caa8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|ItemID|Sentiment|       SentimentText|\n",
      "+------+---------+--------------------+\n",
      "|     1|        0|                 ...|\n",
      "|     2|        0|                 ...|\n",
      "|     3|        1|              omg...|\n",
      "|     4|        0|          .. Omga...|\n",
      "|     5|        0|         i think ...|\n",
      "|     6|        0|         or i jus...|\n",
      "|     7|        1|       Juuuuuuuuu...|\n",
      "|     8|        0|       Sunny Agai...|\n",
      "|     9|        1|      handed in m...|\n",
      "|    10|        1|      hmmmm.... i...|\n",
      "|    11|        0|      I must thin...|\n",
      "|    12|        1|      thanks to a...|\n",
      "|    13|        0|      this weeken...|\n",
      "|    14|        0|     jb isnt show...|\n",
      "|    15|        0|     ok thats it ...|\n",
      "|    16|        0|    &lt;-------- ...|\n",
      "|    17|        0|    awhhe man.......|\n",
      "|    18|        1|    Feeling stran...|\n",
      "|    19|        0|    HUGE roll of ...|\n",
      "|    20|        0|    I just cut my...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a3f6a0-dc3f-4516-baad-1583c498e281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.types.StructType = StructType(StructField(ItemID,IntegerType,true), StructField(Sentiment,IntegerType,true), StructField(SentimentText,StringType,true))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a352c4e-bdca-42da-bffe-abc978bbddd8",
   "metadata": {},
   "source": [
    "### Transform Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c8d8ea-459b-4394-95d7-006cbee60a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "dropRepetitive: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3745/0x000000084156f040@4e3a930f,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "noRepetitiveCharsData: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val dropRepetitive = udf{ str: String => str.replaceAll(\"((.))\\\\1+\",\"$1\").trim.toLowerCase()}\n",
    "val noRepetitiveCharsData = data.withColumn(\"Collapsed\", dropRepetitive('SentimentText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f752aa44-327d-4dd6-b1da-9a7019c131eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|ItemID|Sentiment|       SentimentText|           Collapsed|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|is so sad for my ...|\n",
      "|     2|        0|                 ...|i mised the new m...|\n",
      "|     3|        1|              omg...|omg its already 7...|\n",
      "|     4|        0|          .. Omga...|. omgaga. im so i...|\n",
      "|     5|        0|         i think ...|i think mi bf is ...|\n",
      "|     6|        0|         or i jus...|or i just wory to...|\n",
      "|     7|        1|       Juuuuuuuuu...|        just chilin!|\n",
      "|     8|        0|       Sunny Agai...|suny again work t...|\n",
      "|     9|        1|      handed in m...|handed in my unif...|\n",
      "|    10|        1|      hmmmm.... i...|hm. i wonder how ...|\n",
      "|    11|        0|      I must thin...|i must think abou...|\n",
      "|    12|        1|      thanks to a...|thanks to al the ...|\n",
      "|    13|        0|      this weeken...|this wekend has s...|\n",
      "|    14|        0|     jb isnt show...|jb isnt showing i...|\n",
      "|    15|        0|     ok thats it ...|ok thats it you win.|\n",
      "|    16|        0|    &lt;-------- ...|&lt;- this is the...|\n",
      "|    17|        0|    awhhe man.......|awhe man. i'm com...|\n",
      "|    18|        1|    Feeling stran...|feling strangely ...|\n",
      "|    19|        0|    HUGE roll of ...|huge rol of thund...|\n",
      "|    20|        0|    I just cut my...|i just cut my bea...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noRepetitiveCharsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6a5e3-0ef2-491b-9a75-af8fcafb683f",
   "metadata": {},
   "source": [
    "### Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0104ec9f-ad3c-4373-94ca-4f461defeea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer}\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_a0e14be8b2f5, minTokenLength=1, gaps=true, pattern=\\s+, toLowercase=true\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_cda6e5cbf2ee, binary=false, numFeatures=200000\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_362f86718483\n",
       "tokenized: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 3 more fields]\n",
       "tf: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 4 more fields]\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = IDFModel: uid=idf_362f86718483, numDocs=100000, numFeatures=200000\n",
       "tfidf: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer}\n",
    "\n",
    "// create processing stages\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"Collapsed\")\n",
    "                                    .setOutputCol(\"tokens\")\n",
    "                                    .setPattern(\"\\\\s+\")\n",
    "val hashingTF = new HashingTF().setInputCol(\"tokens\")\n",
    "                               .setOutputCol(\"tf\")\n",
    "                               .setNumFeatures(200000)\n",
    "val idf = new IDF().setInputCol(\"tf\").setOutputCol(\"tfidf\")\n",
    "\n",
    "// tokenize and compute tf\n",
    "val tokenized = tokenizer.transform(noRepetitiveCharsData)\n",
    "val tf = hashingTF.transform(tokenized)\n",
    "\n",
    "// train IDF transformer\n",
    "val idfModel = idf.fit(tf)\n",
    "val tfidf = idfModel.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24547ec2-a148-4372-bcbe-6129d32c8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|               tfidf|\n",
      "+---------+--------------------+\n",
      "|        0|(200000,[36403,48...|\n",
      "|        0|(200000,[16017,26...|\n",
      "|        1|(200000,[24159,40...|\n",
      "|        0|(200000,[3987,761...|\n",
      "|        0|(200000,[27018,55...|\n",
      "|        0|(200000,[80028,10...|\n",
      "|        1|(200000,[100307,1...|\n",
      "|        0|(200000,[3389,339...|\n",
      "|        1|(200000,[4338,748...|\n",
      "|        1|(200000,[10295,15...|\n",
      "|        0|(200000,[112335,1...|\n",
      "|        1|(200000,[6817,775...|\n",
      "|        0|(200000,[24618,30...|\n",
      "|        0|(200000,[28697,32...|\n",
      "|        0|(200000,[4338,139...|\n",
      "|        0|(200000,[6166,160...|\n",
      "|        0|(200000,[23690,23...|\n",
      "|        1|(200000,[12314,48...|\n",
      "|        0|(200000,[26265,37...|\n",
      "|        0|(200000,[12759,16...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datafortraining: org.apache.spark.sql.DataFrame = [Sentiment: int, tfidf: vector]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datafortraining = tfidf.select(\"Sentiment\", \"tfidf\")\n",
    "datafortraining.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f881ec-fa4f-4183-b7ca-ec9f6f7d0d2f",
   "metadata": {},
   "source": [
    "### Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b5ad99-bf19-4e15-8d30-8bf8d9b6fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier, LogisticRegression}\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_e7c21eb022c7\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_e7c21eb022c7, numClasses=2, numFeatures=200000\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier, LogisticRegression}\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"tfidf\").setLabelCol(\"Sentiment\")\n",
    "val lrModel = lr.fit(datafortraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357b30c-fcec-4e4e-9156-140306fd1730",
   "metadata": {},
   "source": [
    "#### Testing on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359d7001-44c4-4240-974b-680c54c89503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|        value|prediction|\n",
      "+-------------+----------+\n",
      "|That is great|       1.0|\n",
      "|That is awful|       0.0|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData: org.apache.spark.sql.DataFrame = [value: string]\n",
       "testTokenized: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 1 more field]\n",
       "testFeatures: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 3 more fields]\n",
       "testLabeled: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testData = Seq(\"That is great\", \"That is awful\").toDF\n",
    "val testTokenized = tokenizer.transform(testData.withColumn(\"Collapsed\", dropRepetitive('value)))\n",
    "val testFeatures = idfModel.transform(hashingTF.transform(testTokenized))\n",
    "val testLabeled = lrModel.transform(testFeatures)\n",
    "testLabeled.select(\"value\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb64a77-9e38-42e7-ab16-ba23e18ae4f3",
   "metadata": {},
   "source": [
    "### Assembling Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e84622-d5c0-4746-81a1-c701c7ded511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "pipe: org.apache.spark.ml.Pipeline = pipeline_bc18b81af4ed\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_bc18b81af4ed\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipe = new Pipeline().setStages(Array(tokenizer, hashingTF, idf, lr))\n",
    "val model = pipe.fit(noRepetitiveCharsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56aa945-eeb7-41aa-978d-a004ad7edc54",
   "metadata": {},
   "source": [
    "#### Testing on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b08ae2f-99e5-49c8-b800-becd606b29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|            value|prediction|\n",
      "+-----------------+----------+\n",
      "|That is not great|       1.0|\n",
      "|That is not awful|       0.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData2: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string]\n",
       "testLabeled2: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testData2 = Seq(\"That is not great\", \"That is not awful\").toDF.withColumn(\"Collapsed\", dropRepetitive('value))\n",
    "val testLabeled2 = model.transform(testData2)\n",
    "testLabeled2.select(\"value\", \"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
