{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a5a6eb-c323-4cac-9e12-310a41f0bac3",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42da1f69-fc0a-4caa-86e5-ecf656813efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0d5d40a6a37e:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1636943122570)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(ItemID,IntegerType,false), StructField(Sentiment,IntegerType,false), StructField(SentimentText,StringType,false))\n",
       "data: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = StructType(\"ItemID Sentiment SentimentText\"\n",
    "                        .split(\" \")\n",
    "                        .map(fieldName => {\n",
    "                            if (fieldName == \"ItemID\" || fieldName == \"Sentiment\")\n",
    "                                StructField(fieldName, IntegerType, nullable = false)\n",
    "                            else\n",
    "                                StructField(fieldName, StringType, nullable = false)\n",
    "                        }))\n",
    "\n",
    "val data = spark.read.format(\"csv\")\n",
    "    .schema(schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"twitter_sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172e0bf6-0926-44d6-81e1-f74b2caa8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|ItemID|Sentiment|       SentimentText|\n",
      "+------+---------+--------------------+\n",
      "|     1|        0|                 ...|\n",
      "|     2|        0|                 ...|\n",
      "|     3|        1|              omg...|\n",
      "|     4|        0|          .. Omga...|\n",
      "|     5|        0|         i think ...|\n",
      "|     6|        0|         or i jus...|\n",
      "|     7|        1|       Juuuuuuuuu...|\n",
      "|     8|        0|       Sunny Agai...|\n",
      "|     9|        1|      handed in m...|\n",
      "|    10|        1|      hmmmm.... i...|\n",
      "|    11|        0|      I must thin...|\n",
      "|    12|        1|      thanks to a...|\n",
      "|    13|        0|      this weeken...|\n",
      "|    14|        0|     jb isnt show...|\n",
      "|    15|        0|     ok thats it ...|\n",
      "|    16|        0|    &lt;-------- ...|\n",
      "|    17|        0|    awhhe man.......|\n",
      "|    18|        1|    Feeling stran...|\n",
      "|    19|        0|    HUGE roll of ...|\n",
      "|    20|        0|    I just cut my...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a3f6a0-dc3f-4516-baad-1583c498e281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.types.StructType = StructType(StructField(ItemID,IntegerType,true), StructField(Sentiment,IntegerType,true), StructField(SentimentText,StringType,true))\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a352c4e-bdca-42da-bffe-abc978bbddd8",
   "metadata": {},
   "source": [
    "### Transform Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c8d8ea-459b-4394-95d7-006cbee60a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n",
       "dropRepetitive: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3743/0x000000084156f840@14b98219,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n",
       "noRepetitiveCharsData: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val dropRepetitive = udf{ str: String => str.replaceAll(\"((.))\\\\1+\",\"$1\").trim.toLowerCase()}\n",
    "val noRepetitiveCharsData = data.withColumn(\"Collapsed\", dropRepetitive('SentimentText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f752aa44-327d-4dd6-b1da-9a7019c131eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|ItemID|Sentiment|       SentimentText|           Collapsed|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|is so sad for my ...|\n",
      "|     2|        0|                 ...|i mised the new m...|\n",
      "|     3|        1|              omg...|omg its already 7...|\n",
      "|     4|        0|          .. Omga...|. omgaga. im so i...|\n",
      "|     5|        0|         i think ...|i think mi bf is ...|\n",
      "|     6|        0|         or i jus...|or i just wory to...|\n",
      "|     7|        1|       Juuuuuuuuu...|        just chilin!|\n",
      "|     8|        0|       Sunny Agai...|suny again work t...|\n",
      "|     9|        1|      handed in m...|handed in my unif...|\n",
      "|    10|        1|      hmmmm.... i...|hm. i wonder how ...|\n",
      "|    11|        0|      I must thin...|i must think abou...|\n",
      "|    12|        1|      thanks to a...|thanks to al the ...|\n",
      "|    13|        0|      this weeken...|this wekend has s...|\n",
      "|    14|        0|     jb isnt show...|jb isnt showing i...|\n",
      "|    15|        0|     ok thats it ...|ok thats it you win.|\n",
      "|    16|        0|    &lt;-------- ...|&lt;- this is the...|\n",
      "|    17|        0|    awhhe man.......|awhe man. i'm com...|\n",
      "|    18|        1|    Feeling stran...|feling strangely ...|\n",
      "|    19|        0|    HUGE roll of ...|huge rol of thund...|\n",
      "|    20|        0|    I just cut my...|i just cut my bea...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noRepetitiveCharsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d6a5e3-0ef2-491b-9a75-af8fcafb683f",
   "metadata": {},
   "source": [
    "### Tokenize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0104ec9f-ad3c-4373-94ca-4f461defeea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer}\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_e7b20beeb502, minTokenLength=1, gaps=true, pattern=\\s+, toLowercase=true\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_3458dd4d0b05, binary=false, numFeatures=200000\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_86c1af0be925\n",
       "tokenized: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 3 more fields]\n",
       "tf: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 4 more fields]\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = IDFModel: uid=idf_86c1af0be925, numDocs=100000, numFeatures=200000\n",
       "tfidf: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer}\n",
    "\n",
    "// create processing stages\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"Collapsed\")\n",
    "                                    .setOutputCol(\"tokens\")\n",
    "                                    .setPattern(\"\\\\s+\")\n",
    "val hashingTF = new HashingTF().setInputCol(\"tokens\")\n",
    "                               .setOutputCol(\"tf\")\n",
    "                               .setNumFeatures(200000)\n",
    "val idf = new IDF().setInputCol(\"tf\").setOutputCol(\"tfidf\")\n",
    "\n",
    "// tokenize and compute tf\n",
    "val tokenized = tokenizer.transform(noRepetitiveCharsData)\n",
    "val tf = hashingTF.transform(tokenized)\n",
    "\n",
    "// train IDF transformer\n",
    "val idfModel = idf.fit(tf)\n",
    "val tfidf = idfModel.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24547ec2-a148-4372-bcbe-6129d32c8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|               tfidf|\n",
      "+---------+--------------------+\n",
      "|        0|(200000,[36403,48...|\n",
      "|        0|(200000,[16017,26...|\n",
      "|        1|(200000,[24159,40...|\n",
      "|        0|(200000,[3987,761...|\n",
      "|        0|(200000,[27018,55...|\n",
      "|        0|(200000,[80028,10...|\n",
      "|        1|(200000,[100307,1...|\n",
      "|        0|(200000,[3389,339...|\n",
      "|        1|(200000,[4338,748...|\n",
      "|        1|(200000,[10295,15...|\n",
      "|        0|(200000,[112335,1...|\n",
      "|        1|(200000,[6817,775...|\n",
      "|        0|(200000,[24618,30...|\n",
      "|        0|(200000,[28697,32...|\n",
      "|        0|(200000,[4338,139...|\n",
      "|        0|(200000,[6166,160...|\n",
      "|        0|(200000,[23690,23...|\n",
      "|        1|(200000,[12314,48...|\n",
      "|        0|(200000,[26265,37...|\n",
      "|        0|(200000,[12759,16...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datafortraining: org.apache.spark.sql.DataFrame = [Sentiment: int, tfidf: vector]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datafortraining = tfidf.select(\"Sentiment\", \"tfidf\")\n",
    "datafortraining.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f881ec-fa4f-4183-b7ca-ec9f6f7d0d2f",
   "metadata": {},
   "source": [
    "### Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b5ad99-bf19-4e15-8d30-8bf8d9b6fd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier, LogisticRegression}\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0e6b821fba18\n",
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid=logreg_0e6b821fba18, numClasses=2, numFeatures=200000\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier, LogisticRegression}\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"tfidf\").setLabelCol(\"Sentiment\")\n",
    "val lrModel = lr.fit(datafortraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357b30c-fcec-4e4e-9156-140306fd1730",
   "metadata": {},
   "source": [
    "#### Testing on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "359d7001-44c4-4240-974b-680c54c89503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|        value|prediction|\n",
      "+-------------+----------+\n",
      "|That is great|       1.0|\n",
      "|That is awful|       0.0|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData: org.apache.spark.sql.DataFrame = [value: string]\n",
       "testTokenized: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 1 more field]\n",
       "testFeatures: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 3 more fields]\n",
       "testLabeled: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testData = Seq(\"That is great\", \"That is awful\").toDF\n",
    "val testTokenized = tokenizer.transform(testData.withColumn(\"Collapsed\", dropRepetitive('value)))\n",
    "val testFeatures = idfModel.transform(hashingTF.transform(testTokenized))\n",
    "val testLabeled = lrModel.transform(testFeatures)\n",
    "testLabeled.select(\"value\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb64a77-9e38-42e7-ab16-ba23e18ae4f3",
   "metadata": {},
   "source": [
    "### Assembling Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20e84622-d5c0-4746-81a1-c701c7ded511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "pipe: org.apache.spark.ml.Pipeline = pipeline_5d1a3ad856fb\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_5d1a3ad856fb\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipe = new Pipeline().setStages(Array(tokenizer, hashingTF, idf, lr))\n",
    "val model = pipe.fit(noRepetitiveCharsData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56aa945-eeb7-41aa-978d-a004ad7edc54",
   "metadata": {},
   "source": [
    "#### Testing on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b08ae2f-99e5-49c8-b800-becd606b29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|            value|prediction|\n",
      "+-----------------+----------+\n",
      "|That is not great|       1.0|\n",
      "|That is not awful|       0.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testData2: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string]\n",
       "testLabeled2: org.apache.spark.sql.DataFrame = [value: string, Collapsed: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testData2 = Seq(\"That is not great\", \"That is not awful\").toDF.withColumn(\"Collapsed\", dropRepetitive('value))\n",
    "val testLabeled2 = model.transform(testData2)\n",
    "testLabeled2.select(\"value\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c5bdc-213a-4bbc-bf98-9fb70ac006f3",
   "metadata": {},
   "source": [
    "### Tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ac2cc5e-8720-41fd-8495-31b06017216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_0e6b821fba18-maxIter: 100,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-20\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 100,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-10\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 100,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-5\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 200,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-20\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 200,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-10\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 200,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-5\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 300,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-20\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 300,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-10\n",
       "}, {\n",
       "\tlogreg_0e6b821fba18-maxIter: 300,\n",
       "\tlogreg_0e6b821fba18-tol: 1.0E-5\n",
       "})\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "      .addGrid(lr.tol, Array(1e-20, 1e-10, 1e-5))\n",
    "      .addGrid(lr.maxIter, Array(100, 200, 300))\n",
    "      .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d4fbc5-5fb1-4d41-b8a6-4107064bab86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_9019dd6e7e13\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "val cv = new CrossValidator()\n",
    "      .setEstimator(pipe)\n",
    "      .setEvaluator(new BinaryClassificationEvaluator()\n",
    "      .setRawPredictionCol(\"prediction\")\n",
    "      .setLabelCol(\"Sentiment\"))\n",
    "      .setEstimatorParamMaps(paramGrid)\n",
    "      .setNumFolds(3) \n",
    "      .setParallelism(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e4ddd24-621d-4a39-9d8d-d108e2f17175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.CrossValidatorModel = CrossValidatorModel: uid=cv_9019dd6e7e13, bestModel=pipeline_5d1a3ad856fb, numFolds=3\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = cv.fit(noRepetitiveCharsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5bca16-e6fc-4bc8-a4b8-01c79198a7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [ItemID: int, Sentiment: int ... 8 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = model.transform(noRepetitiveCharsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ede62f1f-fb91-48d8-b135-fba51b3a8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegParam 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.PipelineModel\n",
       "import org.apache.spark.ml.classification.LogisticRegressionModel\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.PipelineModel\n",
    "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
    "\n",
    "println(\"RegParam \" + model.bestModel.asInstanceOf[PipelineModel].stages(3).asInstanceOf[LogisticRegressionModel].getRegParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d544f1-ac62-4d54-9b76-7100f39240e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
